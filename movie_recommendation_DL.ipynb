{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Deep Learning based Recommender System","metadata":{}},{"cell_type":"markdown","source":"# Data Preprocessing\n\nBefore we start building and training our model, let's do some preprocessing to get the data in the required format.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom tqdm.notebook import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport pytorch_lightning as pl\n\nnp.random.seed(123)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-05-22T16:32:30.407154Z","iopub.execute_input":"2021-05-22T16:32:30.407600Z","iopub.status.idle":"2021-05-22T16:32:30.413576Z","shell.execute_reply.started":"2021-05-22T16:32:30.407549Z","shell.execute_reply":"2021-05-22T16:32:30.412643Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"First, we import the ratings dataset.","metadata":{}},{"cell_type":"code","source":"ratings = pd.read_csv('/kaggle/input/movielens-20m-dataset/rating.csv', \n                      parse_dates=['timestamp'])","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:32:30.415531Z","iopub.execute_input":"2021-05-22T16:32:30.415867Z","iopub.status.idle":"2021-05-22T16:33:31.039464Z","shell.execute_reply.started":"2021-05-22T16:32:30.415838Z","shell.execute_reply":"2021-05-22T16:33:31.038651Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"In order to keep memory usage manageable within Kaggle's kernel, we will only use data from 30% of the users in this dataset. Let's randomly select 30% of the users and only use data from the selected users.","metadata":{}},{"cell_type":"code","source":"rand_userIds = np.random.choice(ratings['userId'].unique(), \n                                size=int(len(ratings['userId'].unique())*0.3), \n                                replace=False)\n\nratings = ratings.loc[ratings['userId'].isin(rand_userIds)]\n\nprint('There are {} rows of data from {} users'.format(len(ratings), len(rand_userIds)))","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:33:31.040889Z","iopub.execute_input":"2021-05-22T16:33:31.041719Z","iopub.status.idle":"2021-05-22T16:33:32.437930Z","shell.execute_reply.started":"2021-05-22T16:33:31.041667Z","shell.execute_reply":"2021-05-22T16:33:32.436761Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"There are 6027314 rows of data from 41547 users\n","output_type":"stream"}]},{"cell_type":"code","source":"ratings.sample(5)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:33:32.440507Z","iopub.execute_input":"2021-05-22T16:33:32.440853Z","iopub.status.idle":"2021-05-22T16:33:32.783416Z","shell.execute_reply.started":"2021-05-22T16:33:32.440821Z","shell.execute_reply":"2021-05-22T16:33:32.782231Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"          userId  movieId  rating           timestamp\n3840312    26182     3704     4.0 2007-01-31 21:56:52\n7608731    52439     3365     4.0 2004-03-21 08:02:56\n19363634  134060     1027     3.0 2003-07-15 22:43:45\n17181947  118860     2629     1.0 2007-11-29 21:27:08\n9344779    64638     4723     2.0 2001-09-10 20:11:41","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>userId</th>\n      <th>movieId</th>\n      <th>rating</th>\n      <th>timestamp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3840312</th>\n      <td>26182</td>\n      <td>3704</td>\n      <td>4.0</td>\n      <td>2007-01-31 21:56:52</td>\n    </tr>\n    <tr>\n      <th>7608731</th>\n      <td>52439</td>\n      <td>3365</td>\n      <td>4.0</td>\n      <td>2004-03-21 08:02:56</td>\n    </tr>\n    <tr>\n      <th>19363634</th>\n      <td>134060</td>\n      <td>1027</td>\n      <td>3.0</td>\n      <td>2003-07-15 22:43:45</td>\n    </tr>\n    <tr>\n      <th>17181947</th>\n      <td>118860</td>\n      <td>2629</td>\n      <td>1.0</td>\n      <td>2007-11-29 21:27:08</td>\n    </tr>\n    <tr>\n      <th>9344779</th>\n      <td>64638</td>\n      <td>4723</td>\n      <td>2.0</td>\n      <td>2001-09-10 20:11:41</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"After filtering the dataset, there are now 6,027,314  rows of data from 41,547 users. Each row in the dataframe corresponds to a movie review made by a single user. ","metadata":{}},{"cell_type":"markdown","source":"### Train-test split\n\nAlong with the rating, there is also a `timestamp` column that shows the date and time the review was submitted. Using the `timestamp` column, we will implement our train-test split strategy using the leave-one-out methodology. For each user, the most recent review is used as the test set (i.e. leave one out), while the rest will be used as training data.\n\nThe code below will split our ratings dataset into a train and test set using the leave-one-out methodology.","metadata":{}},{"cell_type":"code","source":"ratings['rank_latest'] = ratings.groupby(['userId'])['timestamp'] \\\n                                .rank(method='first', ascending=False)\n\ntrain_ratings = ratings[ratings['rank_latest'] != 1]\ntest_ratings = ratings[ratings['rank_latest'] == 1]\n\n# drop columns that we no longer need\ntrain_ratings = train_ratings[['userId', 'movieId', 'rating']]\ntest_ratings = test_ratings[['userId', 'movieId', 'rating']]","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:33:32.785654Z","iopub.execute_input":"2021-05-22T16:33:32.785990Z","iopub.status.idle":"2021-05-22T16:33:34.390953Z","shell.execute_reply.started":"2021-05-22T16:33:32.785958Z","shell.execute_reply":"2021-05-22T16:33:34.389827Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Converting the dataset into an implicit feedback dataset\n","metadata":{}},{"cell_type":"code","source":"train_ratings.loc[:, 'rating'] = 1\n\ntrain_ratings.sample(5)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:33:34.392574Z","iopub.execute_input":"2021-05-22T16:33:34.393102Z","iopub.status.idle":"2021-05-22T16:33:34.840300Z","shell.execute_reply.started":"2021-05-22T16:33:34.393053Z","shell.execute_reply":"2021-05-22T16:33:34.839210Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"          userId  movieId  rating\n3411906    23263     5481       1\n1815983    12245     5464       1\n16198592  112109        6       1\n13914487   96124     1247       1\n1445807     9790     1690       1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>userId</th>\n      <th>movieId</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3411906</th>\n      <td>23263</td>\n      <td>5481</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1815983</th>\n      <td>12245</td>\n      <td>5464</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>16198592</th>\n      <td>112109</td>\n      <td>6</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>13914487</th>\n      <td>96124</td>\n      <td>1247</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1445807</th>\n      <td>9790</td>\n      <td>1690</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"The code below generates 4 negative samples for each row of data. In other words, the ratio of negative to positive samples is 4:1.","metadata":{}},{"cell_type":"code","source":"# Get a list of all movie IDs\nall_movieIds = ratings['movieId'].unique()\n\n# Placeholders that will hold the training data\nusers, items, labels = [], [], []\n\n# This is the set of items that each user has interaction with\nuser_item_set = set(zip(train_ratings['userId'], train_ratings['movieId']))\n\n# 4:1 ratio of negative to positive samples\nnum_negatives = 4\n\nfor (u, i) in tqdm(user_item_set):\n    users.append(u)\n    items.append(i)\n    labels.append(1) # items that the user has interacted with are positive\n    for _ in range(num_negatives):\n        # randomly select an item\n        negative_item = np.random.choice(all_movieIds) \n        # check that the user has not interacted with this item\n        while (u, negative_item) in user_item_set:\n            negative_item = np.random.choice(all_movieIds)\n        users.append(u)\n        items.append(negative_item)\n        labels.append(0) # items not interacted with are negative","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:33:34.841969Z","iopub.execute_input":"2021-05-22T16:33:34.842635Z","iopub.status.idle":"2021-05-22T16:37:47.606722Z","shell.execute_reply.started":"2021-05-22T16:33:34.842564Z","shell.execute_reply":"2021-05-22T16:37:47.605680Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=5985767.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2445e12d1294fe5bf41066523037079"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The class below simply encapsulates the code we have written above into a PyTorch Dataset class.","metadata":{}},{"cell_type":"code","source":"class MovieLensTrainDataset(Dataset):\n    \"\"\"MovieLens PyTorch Dataset for Training\n    \n    Args:\n        ratings (pd.DataFrame): Dataframe containing the movie ratings\n        all_movieIds (list): List containing all movieIds\n    \n    \"\"\"\n\n    def __init__(self, ratings, all_movieIds):\n        self.users, self.items, self.labels = self.get_dataset(ratings, all_movieIds)\n\n    def __len__(self):\n        return len(self.users)\n  \n    def __getitem__(self, idx):\n        return self.users[idx], self.items[idx], self.labels[idx]\n\n    def get_dataset(self, ratings, all_movieIds):\n        users, items, labels = [], [], []\n        user_item_set = set(zip(ratings['userId'], ratings['movieId']))\n\n        num_negatives = 4\n        for u, i in user_item_set:\n            users.append(u)\n            items.append(i)\n            labels.append(1)\n            for _ in range(num_negatives):\n                negative_item = np.random.choice(all_movieIds)\n                while (u, negative_item) in user_item_set:\n                    negative_item = np.random.choice(all_movieIds)\n                users.append(u)\n                items.append(negative_item)\n                labels.append(0)\n\n        return torch.tensor(users), torch.tensor(items), torch.tensor(labels)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:37:47.608143Z","iopub.execute_input":"2021-05-22T16:37:47.608448Z","iopub.status.idle":"2021-05-22T16:37:47.621864Z","shell.execute_reply.started":"2021-05-22T16:37:47.608409Z","shell.execute_reply":"2021-05-22T16:37:47.620972Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Our model - Neural Collaborative Filtering (NCF)\n","metadata":{}},{"cell_type":"code","source":"class NCF(pl.LightningModule):\n    \"\"\" Neural Collaborative Filtering (NCF)\n    \n        Args:\n            num_users (int): Number of unique users\n            num_items (int): Number of unique items\n            ratings (pd.DataFrame): Dataframe containing the movie ratings for training\n            all_movieIds (list): List containing all movieIds (train + test)\n    \"\"\"\n    \n    def __init__(self, num_users, num_items, ratings, all_movieIds):\n        super().__init__()\n        self.user_embedding = nn.Embedding(num_embeddings=num_users, embedding_dim=8)\n        self.item_embedding = nn.Embedding(num_embeddings=num_items, embedding_dim=8)\n        self.fc1 = nn.Linear(in_features=16, out_features=64)\n        self.fc2 = nn.Linear(in_features=64, out_features=32)\n        self.output = nn.Linear(in_features=32, out_features=1)\n        self.ratings = ratings\n        self.all_movieIds = all_movieIds\n        \n    def forward(self, user_input, item_input):\n        \n        # Pass through embedding layers\n        user_embedded = self.user_embedding(user_input)\n        item_embedded = self.item_embedding(item_input)\n\n        # Concat the two embedding layers\n        vector = torch.cat([user_embedded, item_embedded], dim=-1)\n\n        # Pass through dense layer\n        vector = nn.ReLU()(self.fc1(vector))\n        vector = nn.ReLU()(self.fc2(vector))\n\n        # Output layer\n        pred = nn.Sigmoid()(self.output(vector))\n\n        return pred\n    \n    def training_step(self, batch, batch_idx):\n        user_input, item_input, labels = batch\n        predicted_labels = self(user_input, item_input)\n        loss = nn.BCELoss()(predicted_labels, labels.view(-1, 1).float())\n        return loss\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters())\n\n    def train_dataloader(self):\n        return DataLoader(MovieLensTrainDataset(self.ratings, self.all_movieIds),\n                          batch_size=512, num_workers=4)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:37:47.622937Z","iopub.execute_input":"2021-05-22T16:37:47.623230Z","iopub.status.idle":"2021-05-22T16:37:47.646101Z","shell.execute_reply.started":"2021-05-22T16:37:47.623200Z","shell.execute_reply":"2021-05-22T16:37:47.644699Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"We instantiate the NCF model using the class that we have defined above.","metadata":{}},{"cell_type":"code","source":"num_users = ratings['userId'].max()+1\nnum_items = ratings['movieId'].max()+1\n\nall_movieIds = ratings['movieId'].unique()\n\nmodel = NCF(num_users, num_items, train_ratings, all_movieIds)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:37:47.647530Z","iopub.execute_input":"2021-05-22T16:37:47.647893Z","iopub.status.idle":"2021-05-22T16:37:47.763826Z","shell.execute_reply.started":"2021-05-22T16:37:47.647860Z","shell.execute_reply":"2021-05-22T16:37:47.762818Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"Let's train our NCF model for 5 epochs.","metadata":{}},{"cell_type":"code","source":"trainer = pl.Trainer(max_epochs=5,reload_dataloaders_every_epoch=True,\n                     progress_bar_refresh_rate=50, logger=False, checkpoint_callback=False)\n\ntrainer.fit(model)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:44:54.245166Z","iopub.execute_input":"2021-05-22T16:44:54.245550Z","iopub.status.idle":"2021-05-22T19:37:37.193726Z","shell.execute_reply.started":"2021-05-22T16:44:54.245518Z","shell.execute_reply":"2021-05-22T19:37:37.192492Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"GPU available: False, used: False\nTPU available: False, using: 0 TPU cores\n\n  | Name           | Type      | Params\n---------------------------------------------\n0 | user_embedding | Embedding | 1 M   \n1 | item_embedding | Embedding | 1 M   \n2 | fc1            | Linear    | 1 K   \n3 | fc2            | Linear    | 2 K   \n4 | output         | Linear    | 33    \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), maxâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a2bd49b413c45388bfb02b66a202957"}},"metadata":{}},{"name":"stderr","text":"Saving latest checkpoint..\n","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"1"},"metadata":{}}]},{"cell_type":"markdown","source":"# Evaluating our Recommender System\n","metadata":{}},{"cell_type":"markdown","source":"### Hit Ratio @ 10 \n\n","metadata":{}},{"cell_type":"code","source":"# User-item pairs for testing\ntest_user_item_set = set(zip(test_ratings['userId'], test_ratings['movieId']))\n\n# Dict of all items that are interacted with by each user\nuser_interacted_items = ratings.groupby('userId')['movieId'].apply(list).to_dict()\n\nhits = []\nfor (u,i) in tqdm(test_user_item_set):\n    interacted_items = user_interacted_items[u]\n    not_interacted_items = set(all_movieIds) - set(interacted_items)\n    selected_not_interacted = list(np.random.choice(list(not_interacted_items), 99))\n    test_items = selected_not_interacted + [i]\n    \n    predicted_labels = np.squeeze(model(torch.tensor([u]*100), \n                                        torch.tensor(test_items)).detach().numpy())\n    \n    top10_items = [test_items[i] for i in np.argsort(predicted_labels)[::-1][0:10].tolist()]\n    \n    if i in top10_items:\n        hits.append(1)\n    else:\n        hits.append(0)\n        \nprint(\"The Hit Ratio @ 10 is {:.2f}\".format(np.average(hits)))","metadata":{"execution":{"iopub.status.busy":"2021-05-22T19:47:08.197500Z","iopub.execute_input":"2021-05-22T19:47:08.198028Z","iopub.status.idle":"2021-05-22T19:56:33.081878Z","shell.execute_reply.started":"2021-05-22T19:47:08.197980Z","shell.execute_reply":"2021-05-22T19:56:33.080470Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=41547.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b402c58c39fc423a9e21d4087086a54f"}},"metadata":{}},{"name":"stdout","text":"\nThe Hit Ratio @ 10 is 0.87\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We got a pretty good Hit Ratio @ 10 score! To put this into context, what this means is that 87% of the users were recommended the actual item (among a list of 10 items) that they eventually interacted with.","metadata":{}}]}